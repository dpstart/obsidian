topics: #graphnets #transformers #graphtransformers #graphbert

### Why

Overreliance of graphnets on links leads to issues such as *suspended animation* and *oversmoothing*. Moreover, it impedes strong parallelization within the graph, which becomes increasingly important as graphs grow larger.

This model doesn't uses link information, also allowing unsupervised pre-training and fine-tuning/transfer to different graph datasets.


### How


### Results


### What's next
