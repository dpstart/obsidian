topics: #graphnets #transformers #graphtransformers

### Why

Original [[transformer]] from NLP operates on fully-connected graph -> Does not leverage sparse connectivity -> performrs poorly when topology is important + does not leverage edge features if available.

### How


#### Attention

Again, in NLP, a sentence fed to a transformers if treated as a s
#### Positional encodings
#### Layer Norm -> Batch Norm
#### Incorporate edge features

### Results

### What's next